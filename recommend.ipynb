{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata = pd.read_csv(\"app/exports/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('app/exports/collaborative_similarity.pkl', 'rb') as f:\n",
    "    collaborative_similarity = pickle.load(f)\n",
    "\n",
    "with open('app/exports/content_similarity.pkl', 'rb') as f:\n",
    "    content_similarity = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/test_ratings.pkl', 'rb') as f:\n",
    "    test_ratings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movies=None, hide_rated=True, n=20, page=1):\n",
    "    # weights for collaborative and content based similarity\n",
    "    CONTENT_W = 0.2\n",
    "    COLLAB_W = 0.8\n",
    "\n",
    "    movie_similarities = pd.DataFrame()\n",
    "    \n",
    "    # if no movies are rated, return top rated movies\n",
    "    if movies.shape[0] == 0:\n",
    "        movie_similarities = metadata[['movieId', 'order']].sort_values(by='order', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # else calculate similarity scores\n",
    "    else:\n",
    "        # iterate through rated movies\n",
    "        for movie_id, rating in movies.iterrows():\n",
    "            # score with item based collabarative similarity\n",
    "            collaborative_score = collaborative_similarity.get(movie_id, 0)\n",
    "\n",
    "            # score with content based similarity\n",
    "            content_score = content_similarity.get(movie_id, 0)\n",
    "\n",
    "            # add weighted score to dataframe\n",
    "            # similarity = (collaborative_score * COLLAB_W + content_score * CONTENT_W) * (rating - 2.5)\n",
    "            movie_similarities = pd.concat(\n",
    "                [movie_similarities, (collaborative_score * COLLAB_W + content_score * CONTENT_W) * (float(rating) - 2.5)], axis=1)\n",
    "\n",
    "        # sum similarity scores for each movie\n",
    "        movie_similarities = movie_similarities.sum(axis=1)\n",
    "\n",
    "        # hide already rated movies if requested\n",
    "        if hide_rated:\n",
    "            movie_similarities = movie_similarities.drop(movies.index)\n",
    "\n",
    "        # sort similarity with the calculated score\n",
    "        movie_similarities = movie_similarities.sort_values(ascending=False).reset_index().rename(columns={0: 'score', 'index': 'movieId'})\n",
    "        \n",
    "    # make subset of movies based on number of recommendation (n) and page number (page)\n",
    "    movie_similarities = movie_similarities[(page-1)*n:page*n]\n",
    "\n",
    "    return movie_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.405\n",
      "Recall: 0.1643002028397566\n",
      "F1: 0.23376623376623376\n"
     ]
    }
   ],
   "source": [
    "#surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_precision_recall(row):\n",
    "    all_rated = row.dropna()\n",
    "\n",
    "    # get top highest ratings\n",
    "    liked = all_rated.sort_values(ascending=False)[:min(100, all_rated.shape[0])]\n",
    "    liked = liked[liked > 2.5]\n",
    "\n",
    "    # use 50% of the liked movies for testing\n",
    "    actual = liked.sample(frac=0.5, random_state=42)\n",
    "    all_rated.drop(actual.index, inplace=True)\n",
    "\n",
    "    predicted = recommend(pd.DataFrame(all_rated), n=20)\n",
    "\n",
    "    if predicted is None:\n",
    "        return pd.Series([None, None, None])\n",
    "\n",
    "    predicted_ids = predicted['movieId'].tolist()\n",
    "    actual_ids = actual.index.tolist()\n",
    "\n",
    "    tp = len(set(actual_ids) & set(predicted_ids))\n",
    "    fp = len(predicted_ids) - tp\n",
    "    fn = len(actual_ids) - tp\n",
    "\n",
    "    return pd.Series([tp, fp, fn])\n",
    "\n",
    "result = test_ratings.sample(10, random_state=29).apply(get_precision_recall, axis=1)\n",
    "result\n",
    "tp, fp, fn = result.sum()\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
